---
title: "Regression Portfolio"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Multiple Linear Regression

```{r}
## Getting the data
hotelData <- read.csv(file.choose())

head(hotelData)
```

```{r}
## Scatter plot matrix
plot(hotelData[2:6])
```

```{r}
## Correlation table
round(cor(hotelData[2:6]), 2)
```

#### Addressing Multicollinearity
1. Multicollinearity is defined as a condition where two or more predictors are highly correlated with one another. Consequences of multicollinearity can be the instability of coefficient estimates and an inaccurate estimate of the variance
2. As you can see from the scatterplot matrix and the correlation table, there seems to be high correlation between Energy_cons and area as well as numrooms and area
3. The good news is that multicollinearity does not prevent you from obtaining precise predictions, and since this is the aim of our model we can continue without much worry

```{r}

```

```{r}
## Developing a tentative model
model <- lm(Energy_cons ~ area + age + numrooms + OccRate, data=hotelData)
summary(model)
```

```{r}
## Residual Plot
plot(model, which=1)

```
```{r}
## QQ Plot
plot(model, which=2)
```

#### Overall F Test
1. The Overall F Test allows us to check to see if any of the predictors are useful for finding our predicted value, which in this case is 

> $\alpha$ = 0.01 \

> H~0~ : $\beta$~1~ = $\beta$~2~ = $\beta$~3~ = $\beta$~4~ = $\beta$~5~ = 0\
> H~a~ : at least one $\beta$~j~ $\neq$ 0


> Since p < $\alpha$ in this case from the summary above, there is significant evidence to conclude that at least one of the 
> predictors is useful for predicting the response variable, which in this case is energy consumption


```{r}
## Overall F test

```
#### Predictions
1. When doing predictions it is important that most of the time we not predict an X value that is outside of the range of X values that we have, this is called extrapolation. 
    * This can quickly become a problem because we do not know how the data will act outside of the X values we have, and making a prediction on these unknown values could result in errors. 
 






