---
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
.columns {
  display: flex;
}


h1 {
  font-family: "Courier New";
  font-size: 60px
}


h2 {
  font-family: "Courier New";
  font-style: italic;
}
```

# Regression Portfolio

```{r messages=FALSE}
## Libraries used throughout
library(ggplot2)
library(ggcorrplot)
```

## 1. Multiple Linear Regression

Dataset is from the *House Sales in King County, USA* Kaggle Dataset and can be found [here](https://www.kaggle.com/harlfoxem/housesalesprediction)

```{r}
## Getting the data
houseData <- read.csv(file.choose())

plot(houseData[2:17])
```

```{r}
## Correlation plot
corr <- round(cor(houseData[2:17]), 2)
ggcorrplot(
  corr,
  type="lower",
  lab=TRUE,
  lab_size=2.5,
  outline.col="white",
  show.legend=FALSE,
  ggtheme = ggplot2::theme_gray
)
```


#### Addressing Multicollinearity
1. Multicollinearity is defined as a condition where two or more predictors are highly correlated with one another. Consequences of multicollinearity can be the instability of coefficient estimates and an inaccurate estimate of the variance
2. As you can see from the correlation plot, there seems to be high correlation between sqft_living and bathrooms, sqft_above and sqft_living
3. The good news is that multicollinearity does not prevent you from obtaining precise predictions, and since this is the aim of our model we can continue without much worry


```{r}
## Developing a tentative model
initialModel <- lm(price ~  bedrooms + bathrooms + sqft_living + sqft_lot + 
                     floors + waterfront + view + condition + grade +
                     sqft_above + yr_built + yr_renovated + zipcode + 
                     sqft_living15, data=houseData)
summary(initialModel)
```

```{r}
## Creating a reduced model

reducedModel <- lm(price ~  bedrooms + bathrooms + sqft_living + sqft_lot + 
                     floors + waterfront + view + condition + grade + 
                     yr_built + sqft_living15, data=houseData)
summary(reducedModel)
```
```{r}
## Partial F test
anova(initialModel, reducedModel)
```

```{r}
## Residual Plot
plot(reducedModel, which=1)

```

```{r}
## QQ Plot
plot(reducedModel, which=2)
```


#### Predictions
1. When doing predictions it is important that most of the time we not predict an X value that is outside of the range of X values that we have, this is called extrapolation. 
    * This can quickly become a problem because we do not know how the data will act outside of the X values we have, and making a prediction on these unknown values could result in errors. 
 
```{r}
## Creating the new house
newHouse <- data.frame(bedrooms= , bathrooms= , sqft_living= , sqft_lot= , floors= ,  
                       waterfront= , view= , condition= ,  grade= ,  yr_built= ,  sqft_living15= )
```

```{r}
## Predicting the price of the new house
predict(reducedModel, interval="predict", newdata=newHouse)
```



## Logistic Regression


