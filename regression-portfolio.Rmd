---
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}


.columns {
  display: flex;
}


h1 {
  font-family: "Courier New";
  font-size: 60px
}


h2 {
  font-family: "Courier New";
  font-style: italic;
}

.test {
  
}


```

# Regression Portfolio

## 1. Multiple Linear Regression

```{r}
## Getting the data
hotelData <- read.csv(file.choose())

head(hotelData)
```

```{r class.source="test"}
## Scatter plot matrix
plot(hotelData[2:6], pch=16, col="#05668d")
```

```{r}
## Correlation table
round(cor(hotelData[2:6]), 2)
```

#### Addressing Multicollinearity
1. Multicollinearity is defined as a condition where two or more predictors are highly correlated with one another. Consequences of multicollinearity can be the instability of coefficient estimates and an inaccurate estimate of the variance
2. As you can see from the scatterplot matrix and the correlation table, there seems to be high correlation between Energy_cons and area as well as numrooms and area
3. The good news is that multicollinearity does not prevent you from obtaining precise predictions, and since this is the aim of our model we can continue without much worry

```{r}
## Developing a tentative model
model <- lm(Energy_cons ~ area + age + numrooms + OccRate, data=hotelData)
summary(model)
```

```{r}
## Residual Plot
plot(model, which=1)

```
```{r}
## QQ Plot
plot(model, which=2)
```

#### Overall F Test ($\alpha$ = 0.01)

> H~0~ : $\beta$~1~ = $\beta$~2~ = $\beta$~3~ = $\beta$~4~ = $\beta$~5~ = 0\
> H~a~ : at least one $\beta$~j~ $\neq$ 0

Since p (1.828e-05) < $\alpha$, we would reject the null hypothesis, meaning that there is significant evidence to conclude that at least one of the predictors is useful for predicting the response variable, which in this case is energy consumption

#### Partial F Test ($\alpha$ = 0.01)

> H~0~ : $\hat{y}$ = $\beta$~0~ + $\beta$~1~x~1~ + $\beta$~4~x~4~ \
> H~a~ : $\hat{y}$ = $\beta$~0~ + $\beta$~1~x~1~ + $\beta$~2~x~2~ + $\beta$~3~x~3~ + $\beta$~4~x~4~

> n = 19, p = 4, q = 2 \
> F = $(\frac{n-p-1}{q})$ $(\frac{R^2_{1} - R^2_{2}}{1 - R^2_{1}})$

```{r}
## Creating the reduced model for the partial F test
reducedModel <- lm(Energy_cons ~ area + OccRate, data=hotelData)
summary(reducedModel)
```
> F = $(\frac{19-4-1}{2})$ $(\frac{0.8402 - 0.8223}{1 - 0.8402})$ = 0.7841

```{r}
## Getting the p value
1 - pf(0.7841, 2, 14)
```
Since p > $\alpha$, we fail to reject the null hypothesis, meaning that there is not sufficient evidence to say that the full model is better than the reduced model so we can use the reduced model for our predictions

#### The Final Model
> $\hat{y}$ = -55.19 + 0.001679x~1~ + 74.19x~4~


#### Predictions
1. When doing predictions it is important that most of the time we not predict an X value that is outside of the range of X values that we have, this is called extrapolation. 
    * This can quickly become a problem because we do not know how the data will act outside of the X values we have, and making a prediction on these unknown values could result in errors. 
 






